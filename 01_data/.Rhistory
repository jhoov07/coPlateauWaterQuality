sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(9:93)]),]
As_testComp<- As_test[complete.cases(As_test[,c(9:93)]),]
#define predictor and response variables in training set
train_x<-data.matrix(As_trainComp[, -c(1:8)])
train_y<-As_trainComp[,4]
#define predictor and response variables in testing set
test_x<-data.matrix(As_testComp[, -c(1:8)])
test_y<-As_testComp[,4]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x)
xgb_test = xgb.DMatrix(data = test_x)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 10)
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
#load the data
data = MASS::Boston
#Clean up the workspace
rm(list=ls())
##Test Code
#load the data
data = MASS::Boston
#view the structure of the data
str(data)
#make this example reproducible
set.seed(0)
#split into training (80%) and testing set (20%)
parts = createDataPartition(data$medv, p = .8, list = F)
train = data[parts, ]
test = data[-parts, ]
#define predictor and response variables in training set
train_x = data.matrix(train[, -13])
train_y = train[,13]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -13])
test_y = test[, 13]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
View(test_x)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 10)
#Clean up the workspace
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
View(Asdata)
#Clean up the workspace
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
Asdata<-as.data.frame(Asdata)
# get the numb 70/30 training test split
#split into training (70%) and testing set (30%)
sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(1,9:93)]),]
As_testComp<- As_test[complete.cases(As_test[,c(1,9:93)]),]
#define predictor and response variables in training set
train_x<-data.matrix(As_trainComp[, -c(1:8)])
train_y<-As_trainComp[,1]
#define predictor and response variables in testing set
test_x<-data.matrix(As_testComp[, -c(1:8)])
test_y<-As_testComp[,1]
View(All_Asdata)
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(1,9:93)]),]
As_testComp<- As_test[complete.cases(As_test[,c(1,9:93)]),]
#define predictor and response variables in training set
train_x<-data.matrix(As_trainComp[, -c(1:8)])
train_y<-As_trainComp[,1]
#define predictor and response variables in testing set
test_x<-data.matrix(As_testComp[, -c(1:8)])
test_y<-As_testComp[,1]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x)
View(All_Asdata)
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
#Asdata<-as.data.frame(Asdata)
# get the numb 70/30 training test split
#split into training (70%) and testing set (30%)
sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(3,9:93)]),]  #col 3, testing on As >10 ug/L category
As_testComp<- As_test[complete.cases(As_test[,c(3,9:93)]),] #col 3testing on As >10 ug/L category
#define predictor and response variables in training set
train_x<-data.matrix(As_trainComp[, -c(1:8)])
train_y<-As_trainComp[,3]
#define predictor and response variables in testing set
test_x<-data.matrix(As_testComp[, -c(1:8)])
test_y<-As_testComp[,3]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y).  #OK, this runs when the outcome variable is numeric
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)  #OK, this runs when the outcome variable is numeric
xgb_test = xgb.DMatrix(data = test_x)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 10)
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)  #OK, this runs when the outcome variable is numeric
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 10)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 50)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 100)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 3, watchlist=watchlist, nrounds = 200)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 4, watchlist=watchlist, nrounds = 70)
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 4, watchlist=watchlist, nrounds = 70, bjective = "binary:logistic")
?xgb.train
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, max.depth = 4, watchlist=watchlist, nrounds = 70, objective = "binary:logistic")
library(DMwR)
install.packages("DMwR2")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library(tidyverse) # general utility functions
library(DMwR2)
setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/01_data")
#setwd("C:/Users/austinmartinez/Documents/GitHub/coPlateauWaterQuality/01_data")
#Clean up the workspace
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
#Asdata<-as.data.frame(Asdata)
# get the numb 70/30 training test split
#split into training (70%) and testing set (30%)
sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(3,9:93)]),]  #col 3, testing on As >10 ug/L category
As_testComp<- As_test[complete.cases(As_test[,c(3,9:93)]),] #col 3testing on As >10 ug/L category
?SMOTE
#Use SMOTE to balance the trianing dataset
As_trainComp<-SMOTE(bas10~.,
data.frame(As_trainComp),
perc.over = 100,
perc.under = 200)
install.packages("smotefamily")
library(smotefamily)
#Use SMOTE to balance the trianing dataset
As_trainComp<-SMOTE(bas10~.,
data.frame(As_trainComp),
perc.over = 100,
perc.under = 200)
#Use SMOTE to balance the trainning dataset
As_trainComp<-SMOTE(bas10~.,
data.frame(As_trainComp[,c(3,9:93)]),
perc.over = 100,
perc.under = 200)
?SMOTE
#Use SMOTE to balance the trainning dataset
As_trainComp<-SMOTE(bas10~.,
data.frame(As_trainComp[,c(3,9:93)]))
View(As_train)
#Use SMOTE to balance the trainning dataset
As_trainComp<-SMOTE(X=As_train[,c(3,9:93)],
target = 'bas10'))
#Use SMOTE to balance the trainning dataset
As_trainComp<-SMOTE(X=As_train[,c(3,9:93)],
target = 'bas10')
#Use SMOTE to balance the trainning dataset
As_trainComp<-SMOTE(X=As_train[,c(3,9:93)],
target = 'bas10', K=5)
?xgb.train
?train
#Clean up the workspace
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
#Asdata<-as.data.frame(Asdata)
# get the numb 70/30 training test split
#split into training (70%) and testing set (30%)
sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(3,9:93)]),]  #col 3, testing on As >10 ug/L category
As_testComp<- As_test[complete.cases(As_test[,c(3,9:93)]),] #col 3testing on As >10 ug/L category
#define predictor and response variables in training set
train_x<-data.matrix(As_trainComp[, -c(1:8)])
train_y<-As_trainComp[,3]
#define predictor and response variables in testing set
test_x<-data.matrix(As_testComp[, -c(1:8)])
test_y<-As_testComp[,3]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)  #OK, this runs when the outcome variable is numeric
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
model<-train(
bas10 ~ .,
data = xgb_train,
metric = "Accuracy",
method = "xgbTree"
)
model<-train(
bas10 ~ .,
data = train_x,
metric = "Accuracy",
method = "xgbTree"
)
model<-train(
bas10 ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree"
)
As_trainComp <- As_train[,c(3,9:93)] #col 3, testing on As >10 ug/L category
#Clean up the workspace
rm(list=ls())
#Load data
All_Asdata = read.csv("AsModelInput.csv")
# set a random seed & shuffle data frame
set.seed(1234)
Asdata <- All_Asdata[sample(1:nrow(All_Asdata)), ]
#Asdata<-as.data.frame(Asdata)
# get the numb 70/30 training test split
#split into training (70%) and testing set (30%)
sample_set<-sample(nrow(Asdata), round(nrow(Asdata)*.7), replace = F)
As_train = Asdata[sample_set,]
As_test = Asdata[-sample_set,]
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(3,9:93)]),]  #col 3, testing on As >10 ug/L category
As_testComp<- As_test[complete.cases(As_test[,c(3,9:93)]),] #col 3testing on As >10 ug/L category
As_trainComp <- As_train[,c(3,9:93)] #col 3, testing on As >10 ug/L category
As_testComp<- As_test[,c(3,9:93)]) #col 3testing on As >10 ug/L category
As_testComp<- As_test[,c(3,9:93)] #col 3testing on As >10 ug/L category
model<-train(
bas10 ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree"
)
model<-train(
bas10 ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="none"),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
summary(As_trainComp)
#Complete cases
As_trainComp <- As_train[complete.cases(As_train[,c(3,9:93)]),]  #col 3, testing on As >10 ug/L category
As_testComp<- As_test[complete.cases(As_test[,c(3,9:93)]),] #col 3testing on As >10 ug/L category
As_trainComp <- As_trainComp[,c(3,9:93)] #col 3, testing on As >10 ug/L category
As_testComp<- As_testComp[,c(3,9:93)] #col 3testing on As >10 ug/L category
model<-train(
bas10 ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="none"),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="none"),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv"),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv"),
number = 5,
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 5),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = 6,
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model
model$resample %>%
arrange(Resample)
?seq
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 13),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = seq(8, 14, by=2),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 3),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = seq(8, 14, by=4),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model$resample %>%
arrange(Resample)
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 3),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = seq(from = 8, to = 14, by=4),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model$resample %>%
arrange(Resample)
model
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 5),
tuneGrid = expand.grid(
nrounds = 100,
max_depth = seq(from = 8, to = 14, by=2),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model
expand.grid(
nrounds = seq(from = 500, to = 2000, by = 500),
max_depth = seq(from = 8, to = 14, by = 2),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
x<-expand.grid(
x<-expand.grid(
nrounds = seq(from = 500, to = 2000, by = 500),
max_depth = seq(from = 8, to = 14, by = 2),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1)
x
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 3),
tuneGrid = expand.grid(
nrounds = seq(from = 500, to = 2000, by = 500),
max_depth = seq(from = 8, to = 14, by = 2),
eta = 0.3,
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
)
model
model$resample %>%
arrange(Resample)
model<-train(
factor(bas10) ~ .,
data = As_trainComp,
metric = "Accuracy",
method = "xgbTree",
trControl = trainControl(method="cv", number = 3),
tuneGrid = expand.grid(
nrounds = seq(from = 500, to = 2000, by = 500),
max_depth = seq(from = 8, to = 14, by = 2),
eta = 0.3,
gamma = 0,
colsample_bytree = seq(from = 0.25, to = 0.75, by = 0.25),
min_child_weight = 1,
subsample = 1
)
)
model
model$resample %>%
arrange(Resample)
