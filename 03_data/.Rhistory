library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#highest accuracy is 0.755 using 20 variables with the highest gain values - from the csv output from step 3
a<-list("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr",
"C_Kaolinit", "Top5_As", "Top5_Ba", "C_U")
#For 21 Variables and 0.757 Accuracy, add C_U and 93
#define predictor and response variables in training set, As= 5 ug/L, keep variables defined above
train_x = data.matrix(train[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9,93)])
#train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[,160]
#define predictor and response variables in testing set
test_x = data.matrix(test[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9,93)])
test_y = test[, 160]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Use fully tuned hyperparameters from steps 1 and 2
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 1,
gamma = 2,
max_delta_step = 0,
eta = 0.01,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.75,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
#write.csv(dfAc, file="20241223_as5ugL_modelTuning_primaryHyperparameters.csv")
#Testing Data
xgbpred <- predict (model, xgb_test)
xgbpred2 <- ifelse (xgbpred > 0.5,1,0)
confusionMatrix (factor(xgbpred2), factor(test_y))
#Compare training and testing accuracy to the model with all variables and tuned hyperparameters - from Step 2
#Rerun with different variable subset if needed, might take some tinkering to identify the correct number of variables to keep
#Make note of the variables to keep then go to script 5 and run the final model and calculate model metrics
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
ts_data = read.csv("/5ug/L_XGBTuning/2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
ts_data = read.csv("/5ug/L_XGBTuning/2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/ug/L_XGBTuning")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/ugL_XGBTuning")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/ugL_XGBTuning")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning")
rm(list=ls())
ts_data = read.csv("2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
# Install and load required packages
install.packages("forecast")
library(forecast)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 3)
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning")
rm(list=ls())
ts_data = read.csv("2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
# Install and load required packages
install.packages("forecast")
library(forecast)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 5)
install.packages("forecast")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning")
rm(list=ls())
ts_data = read.csv("2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
# Install and load required packages
#install.packages("forecast")
library(forecast)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 5)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 4)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 3)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 2)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data, n = 1)
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning")
rm(list=ls())
ts_data = read.csv("2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
# Install and load required packages
#install.packages("forecast")
library(forecast)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data[,3])
# Plot the original time series and the moving average
plot(ts_data, col = "blue", main = "Time Series with Simple Moving Average")
lines(sma_result, col = "red")
legend("topright", legend = c("Original", "Simple Moving Average"),
col = c("blue", "red"), lty = 1)
#Make a plot to show how accuracy varies by number of variables
ggplot(dfMetrics, aes(x=i, y=Test_Error))+geom_line()
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning")
rm(list=ls())
ts_data = read.csv("2025128_as5XGB_variableDrop_accuracySDImpacts.csv", na.strings = "NULL")
# Install and load required packages
#install.packages("forecast")
library(forecast)
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(ts_data[,3])
# Plot the original time series and the moving average
plot(ts_data, col = "blue", main = "Time Series with Simple Moving Average")
lines(sma_result, col = "red")
legend("topright", legend = c("Original", "Simple Moving Average"),
col = c("blue", "red"), lty = 1)
data<-read.csv("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/5ugL_XGBTuning/2025128_as5XGB_variableDrop_accuracySDImpacts.csv")
dataTS<-ts(data[,3])
# Calculate a simple moving average with a window size of 3
sma_result <- TTR::SMA(dataTS, n = 5)
# Plot the original time series and the moving average
plot(dataTS, col = "blue", main = "Time Series with Moving Average")
lines(sma_result, col = "red")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr",
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr",
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr",
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr",
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
#("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz", "Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr", "C_Kaolinit", "Top5_As", "Top5_Ba", "C_U")
#(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9,93)
#highest accuracy is 0.755 using 20 variables with the highest gain values - from the csv output from step 3
a<-list("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz")
#For 21 Variables and 0.757 Accuracy, add C_U and 93
#define predictor and response variables in training set, As= 5 ug/L, keep variables defined above
train_x = data.matrix(train[, c(3,2,5,27,108,38,88,87,47)])
#train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[,160]
#define predictor and response variables in testing set
test_x = data.matrix(test[, c(3,2,5,27,108,38,88,87,47)])
test_y = test[, 160]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Use fully tuned hyperparameters from steps 1 and 2
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 1,
gamma = 2,
max_delta_step = 0,
eta = 0.01,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.75,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
#write.csv(dfAc, file="20241223_as5ugL_modelTuning_primaryHyperparameters.csv")
#Testing Data
xgbpred <- predict (model, xgb_test)
xgbpred2 <- ifelse (xgbpred > 0.5,1,0)
confusionMatrix (factor(xgbpred2), factor(test_y))
#Compare training and testing accuracy to the model with all variables and tuned hyperparameters - from Step 2
#Rerun with different variable subset if needed, might take some tinkering to identify the correct number of variables to keep
#Make note of the variables to keep then go to script 5 and run the final model and calculate model metrics
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
library(cutpointr)
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
a<-list("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr", "C_Kaolinit",
"Top5_As", "Top5_Ba")
#define predictor and response variables in training set, As= 5 ug/L, keep variables defined above
train_x = data.matrix(train[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9)])
train_y = train[,160]
#define predictor and response variables in testing set
test_x = data.matrix(test[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9)])
test_y = test[,160]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Set parameters from all the tuning, steps 2 and 3
params = list(alpha = 0,
lambda = 1,
gamma = 2,
max_delta_step = 0,
eta = 0.001,
max_depth = 4,
subsample = 0.50,
colsample_bytree = 0.75,
min_child_weight = 1,
booster = "gbtree")
#Fully tuned model
model = xgboost(data = xgb_train, params = params,
nrounds = 1000, objective = "binary:logistic",
eval_metric = "error", verbose = 1,
print_every_n = 100)
#write.csv(dfAc, file="20241223_modelTuning_primaryHyperparameters_alpha2Lambda5.csv")
#Testing Data
xgbpred <- predict (model, xgb_test)
xgbpred2 <- ifelse (xgbpred > 0.5,1,0)
confusionMatrix (factor(xgbpred2), factor(test_y)) #keep this for reporting
#Adjust the "true" threshold using Youden value
#For a figure
y_predJoin<-data.frame(cbind(test_y, xgbpred))#change field to match outcome modeled, this applies to LT10
#rename fields for ease of use
colnames(y_predJoin)[1]<-"Obsclass"
colnames(y_predJoin)[2]<-"Predexceed"
#Use cutpoint to identify threshold for As 'detection' balancing sensitivity and specificity using Youden metric
cp <- cutpointr(y_predJoin, Predexceed, Obsclass,
method = maximize_metric, metric = youden, pot_class = 1)
summary(cp) #make note of the cutpoint value for comparision with lines 91-93 above
plot(cp)
#Extract ROC Curve data for plotting
a<-as.data.frame(cp$roc_curve)
a$sens<-a$tp/(a$tp+a$fn) #sensitivity
a$spec<-a$tn/(a$tn+a$fp) #specificity
a$j<-(a$tp/(a$tp+a$fn))+(a$tn/(a$tn+a$fp))-1 #j-index, also called Youden value
##Make a plot like USGS PFAS paper S8
df <- a %>%
select(x.sorted, j, sens, spec) %>%
gather(key = "variable", value = "value", -x.sorted)
ggplot(df, aes(x = x.sorted, y = value)) +
geom_line(aes(color = variable, linetype = variable)) +
scale_color_manual(values = c("black","darkred", "steelblue")) +
xlab("As Detection Threshold - value above this threshold is considered a detection") + ylab("Metric Estimate")
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = model, X_train = train_x)
# **SHAP summary plot**
shap.plot.summary(shap_long)
