date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 5 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 2,
lambda = 1,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_alpha2.csv")
# load libraries
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 5 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 3,
lambda = 1,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_alpha3.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 5 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 4,
lambda = 1,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_alpha4.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 10 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 5,
lambda = 1,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_alpha5.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 10 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 2,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_lambda2.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 10 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 3,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_lambda3.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 10 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 4,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_lambda4.csv")
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
#library(tidyverse) # general utility functions
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#define predictor and response variables in training set, As= 10 ug/L
train_x = data.matrix(train[, -c(1, 4, 109:112, 157:168)])
train_y = train[, 161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -c(1, 4, 109:112, 157:168)])
test_y = test[, 161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Run model 10 times and calculate accuracy and SD of accuracy, change hyperparameter value as needed
dfAc<-data.frame()
params = list(alpha = 0,
lambda = 5,
gamma = 0,
max_delta_step = 0,
eta = 0.0075,
max_depth = 4,
subsample = 0.5,
colsample_bytree = 0.5,
min_child_weight = 1,
booster = "gbtree")
##XGB Train
for(data in 1:10){
model = xgb.train(data = xgb_train, params = params,
watchlist = watchlist,
nrounds = 1000, objective = "binary:logistic",
eval_metric = list("error"), verbose = 1,
print_every_n = 100)
x<-1-last(model$evaluation_log$train_error)
y<-1-last(model$evaluation_log$test_error)
xy<-cbind(x,y); print(xy)
dfAc<-rbind(dfAc, xy)
}
#Clean up and write to file
colnames(dfAc)[1]<-"Train_Error"
colnames(dfAc)[2]<-"Test_Error"
mean(dfAc$Train_Error)
sd(dfAc$Train_Error)
mean(dfAc$Test_Error)
sd(dfAc$Test_Error)
write.csv(dfAc, file = "~/Desktop/2025116_as10ugL_modelTuning_primaryHyperparameters_lambda5.csv")
