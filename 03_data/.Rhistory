prism30yr = PRISM_30yrNorm,
welldpth = WellDepthMeasureValue,
As = ResultMeasureValue,
F30mElevationFoCo = F30mElevat)
#Rename fields in NNwells data
NNwells<- NNwells %>%
rename(welldpth = depth,
As = As_,
Fl = Fl_combine)
#table(data3$Fl)
##Concatenate data source name to SiteID/WellID/Record Number
Nure$SiteID<-paste("nure-",Nure$rec_no, sep="")
WQP$SiteID<-paste("nnWells-",WQP$SiteID, sep="")
NNwells$SiteID<-paste("wtrQalPort-",NNwells$well_id, sep="")
str(data3$SiteID)
str(NNwells$SiteID)
#Need to find missing fields then create new blank fields, i started by comparing data and data1, need to repeat for data2
a<-colnames(Nure)
b<-colnames(WQP)
c<-colnames(NNwells)
setdiff(a, b) #this will tell you the fields that don't match between a and b, anything not matching needs to be added to b
setdiff(b, a) #this will tell you the fields that don't match between b and a, anything not matching needs to be added to a
setdiff(a, c)
setdiff(c, b)
#setwd("~/Desktop")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality")
#Load libraries
library(tidyverse)
library(dplyr)
#Clean up the workspace
rm(list=ls())
#Load data
Nure <- read.csv("./02_uranium/01_data/Nure7_Data_ExportTable.csv")
WQP <- read.csv("./02_uranium/01_data/wqpData_cleaned_20240808.csv", na.strings = "NULL")
NNwells <- read.csv("./02_uranium/01_data/nnwells3_Check_ExportTable.csv")
#Rename fields in NURE dataset
Nure <- Nure %>%
rename(baseflow = bfi48grd_ProjectRaster2, prism30yr = PRISM_ppt_30yr_ProjectRaster1, U = u_fl_ppb)
#Rename fields in WQP Data
WQP<- WQP %>%
rename(baseflow = bfi48grd,
prism30yr = PRISM_30yrNorm,
welldpth = WellDepthMeasureValue,
As = ResultMeasureValue,
F30mElevationFoCo = F30mElevat)
#str(NNwells$SiteID)
#combine data sets
combined_data <- bind_rows(Nure, WQP)
combined_data1 <- bind_rows(combined_data, NNwells)#
#Need to find missing fields then create new blank fields, i started by comparing data and data1, need to repeat for data2
a<-colnames(Nure)
b<-colnames(WQP)
c<-colnames(NNwells)
setdiff(c, a)
setdiff(c, b)
setdiff(a, b) #this will tell you the fields that don't match between a and b, anything not matching needs to be added to b, shows what data is in a that is not in b
setdiff(b, a) #this will tell you the fields that don't match between b and a, anything not matching needs to be added to a
setdiff(a, c)
setdiff(b, c)
setdiff(c, b)
setdiff(c, a)
#Load libraries
library(reshape2)
library(dplyr)
library(tidyr)
#Set working directory
setwd("/Volumes/HooverShare/Shared_Group_Data/20_projects/06_coPlateau_Rework/")
#Set working directory
setwd("/Volumes/HooverShare/Shared_Group_Data/20_projects/06_coPlateau_Rework/")
#Set working directory
setwd("/Volumes/HooverShare/Shared_Group_Data/20_projects/06_coPlateau_Rework/")
getwd()
#Set working directory
setwd("/Volumes/HooverShare/Shared_Group_Data/20_projects/06_coPlateau_Rework/")
setwd("\\10.192.74.47\HooverShare\Shared_Group_Data")
setwd("//10.192.74.47/HooverShare/Shared_Group_Data")
setwd("/10.192.74.47/HooverShare/Shared_Group_Data")
#Set working directory
setwd("/Volumes/HooverShare/Shared_Group_Data/20_projects/06_coPlateau_Rework/")
#Clean up the workspace
rm(list=ls())
#Load csv files
a<-read.csv("./02_Data/Raw_Data/WQP/00_archive/AZ_Uranium.csv", na.strings = "NULL")
b<-read.csv("./02_Data/Raw_Data/WQP/00_archive/NM_Uranium.csv", na.strings = "NULL")
c<-read.csv("./02_Data/Raw_Data/WQP/00_archive/Ca.csv", na.strings = "NULL")
d<-read.csv("./02_Data/Raw_Data/WQP/00_archive/Iron.csv", na.strings = "NULL")
e<-read.csv("./02_Data/Raw_Data/WQP/00_archive/ph.csv", na.strings = "NULL")
f<-read.csv("./02_Data/Raw_Data/WQP/00_archive/alkalinity.csv", na.strings = "NULL")
g<-read.csv("./02_Data/Raw_Data/WQP/00_archive/CO_Uranium.csv", na.strings = "NULL")
h<-read.csv("./02_Data/Raw_Data/WQP/00_archive/UT_Uranium.csv", na.strings = "NULL")
u<-rbind(a,b,g,h)
#Process uranium data (done)
u2 <- u %>%
drop_na(ResultMeasureMeasureUnitCode) %>%
filter(ResultSampleFractionText == "Dissolved") %>%
filter(ResultMeasureMeasureUnitCode != "ratio") %>%
filter(CharacteristicName == "U")
summary(factor(u2$ResultMeasureMeasureUnitCode)) #Check to see what units are noted in the field
mgL_indices <- which(u2$ResultMeasureMeasureUnitCode == "pCi/L") #Create an index with records that we need to convert
u2$ResultMeasureValue[mgL_indices] <- u2$ResultMeasureValue[mgL_indices] * 0.67 #Convert to pCi/L to ug/L match NN Wells analyte data
u2$ResultMeasureMeasureUnitCode <- "ug/L"   # made all units mg/L
#Calcium: keep mg/L (done)
c2 <- c %>%
drop_na(ResultMeasureMeasureUnitCode) %>%
filter(ResultSampleFractionText == "Dissolved")
summary(factor(c2$ResultMeasureMeasureUnitCode)) #Check to see what units are noted in the field
mgL_indices <- which(c2$ResultMeasureMeasureUnitCode == "ug/l" | c2$ResultMeasureMeasureUnitCode == "ug/L") #Create an index with records that we need to convert
c2$ResultMeasureValue[mgL_indices] <- c2$ResultMeasureValue[mgL_indices] / 1000 #Convert to mg/L to match NN Wells analyte data
c2$ResultMeasureMeasureUnitCode <- "mg/L"   # made all units mg/L
#Iron: unit conversions (done)
d2 <- d %>%
drop_na(ResultMeasureMeasureUnitCode) %>%
filter(ResultSampleFractionText == "Dissolved") %>%
filter(CharacteristicName == "Fe") #remove Ferric ion, Ferrous ion, and Iron-59
summary(factor(d2$ResultMeasureMeasureUnitCode)) #Check to see what units are noted in the field
mgL_indices <- which(d2$ResultMeasureMeasureUnitCode == "mg/l" | d2$ResultMeasureMeasureUnitCode == "mg/L") #Create an index with records that we need to convert
d2$ResultMeasureValue[mgL_indices] <- d2$ResultMeasureValue[mgL_indices] * 1000 #Convert to ug/L to match NN Wells analyte data
d2$ResultMeasureMeasureUnitCode <- "ug/L"   # made all units ug/L
#pH - check that all measurements are in standard units (done)
e2 <- e %>%
drop_na(ResultMeasureMeasureUnitCode) %>%
filter(ResultSampleFractionText == "Total")
#summary(factor(e2$ResultMeasureMeasureUnitCode))
#Alkalinity: convert ug/L to mg/L in ResultMeasureMeasureUnitCode (done)
f2 <- f %>%
drop_na(ResultMeasureMeasureUnitCode) %>%   #Remove rows with NA's using drop_na()
filter(ResultSampleFractionText == "Total") %>% #filter to total results since we want to use raw water samples
filter(CharacteristicName == "Alkalinity")
summary(factor(f2$ResultMeasureMeasureUnitCode)) #Check to see what units are noted in the field
mgL_indices <- which(f2$ResultMeasureMeasureUnitCode == "ug/l") #Create an index with records that we need to convert
f2$ResultMeasureValue[mgL_indices] <- f2$ResultMeasureValue[mgL_indices] / 1000 #Convert to ug/L to match NN Wells analyte data
f2$ResultMeasureMeasureUnitCode <- "mg/L"   # made all units mg/L
#Merge files
cdef<-rbind(c2,d2,e2,f2, u2)
cdef2 <- cdef %>%
filter(StateCode == 4 & (CountyCode == 1 | CountyCode == 5 | CountyCode == 7 | CountyCode == 15 | CountyCode == 17 | CountyCode == 25) |
(StateCode == 35 & (CountyCode == 3 | CountyCode ==6 | CountyCode ==31 | CountyCode ==39 | CountyCode ==43 | CountyCode ==45)) |
(StateCode == 08 & (CountyCode == 29 | CountyCode == 33 | CountyCode == 45 | CountyCode ==67 | CountyCode ==77 | CountyCode ==81 | CountyCode ==83 | CountyCode ==85 | CountyCode ==91 | CountyCode ==103 | CountyCode ==113)) |
(StateCode == 49 & (CountyCode == 1 | CountyCode == 7 | CountyCode ==13 | CountyCode ==15 | CountyCode ==17 | CountyCode == 19 | CountyCode ==21 | CountyCode ==23 | CountyCode ==25 | CountyCode ==27 | CountyCode ==31 | CountyCode == 37 | CountyCode ==39 | CountyCode ==41 | CountyCode ==47 | CountyCode ==49 | CountyCode ==51 | CountyCode == 53 | CountyCode ==55)))
#convert to wide format
wide<-dcast(cdef2, SiteID~CharacteristicName+ResultMeasureMeasureUnitCode, value.var="ResultMeasureValue", median)
View(wide)
#convert to wide format
wide<-dcast(cdef2, SiteID~CharacteristicName, value.var="ResultMeasureValue", median)
View(wide)
#Read GIS data from WQP
i<-read.csv("./02_Data/Raw_Data/WQP/00_archive/20241029_WQP_Export.csv", na.strings = "NULL")
View(i)
#Clean up new dataframe, drop worthless fields
cleani<- i [-c(2:25,27:79)]
View(cleani)
#class(cleani[["SiteID"]])
#class(wide[["SiteID"]])
#Merge with wide using SiteID
WQP_All<-merge(wide, cleani, by="SiteID", all.x=TRUE)
View(WQP_All)
View(WQP_All)
View(WQP_All)
#class(cleani[["SiteID"]])
#class(wide[["SiteID"]])
#Merge with wide using SiteID
WQP_All<-merge(wide, cleani, by="SiteID", all.y=TRUE)
#class(cleani[["SiteID"]])
#class(wide[["SiteID"]])
#Merge with wide using SiteID
WQP_All<-merge(wide, cleani, by="SiteID", all.x=TRUE)
?distinct
#Return unique records
yourDataFrame <- i %>%
distinct(cleani .keep_all = TRUE)
#Return unique records
yourDataFrame <- i %>%
distinct(cleani, .keep_all = TRUE)
#Return unique records
df <- i %>%
distinct(cleani[, 1:155], .keep_all = TRUE)
#Return unique records
df <- i %>%
distinct(cleani[, 1:155])
#Return unique records
df <- i %>%
distinct(cleani[, 1:155], .keep_all = FALSE)
#Return unique records
cleanNoDup<-distinct(cleani)
View(cleanNoDup)
#Return unique records
cleanNoDup<-distinct(cleani[,c(2:155)])
View(cleanNoDup)
#class(cleani[["SiteID"]])
#class(wide[["SiteID"]])
#Merge with wide using SiteID
WQP_All<-merge(wide, cleani, by="SiteID", all.x=TRUE)
View(WQP_All)
#class(cleani[["SiteID"]])
#class(wide[["SiteID"]])
#Merge with wide using SiteID
WQP_All<-merge(wide, cleanNoDup, by="SiteID", all.x=TRUE)
View(WQP_All)
version()
version
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
library(cutpointr)
#library(tidyverse)
#for spatial data
library(raster)
library(sp)
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
#setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
a<-list("pH", "prism30yr", "A_Cs", "A_Aragon", "C_Hematite", "Fe", "Top5_S", "C_Cr", "A_Calcite",
"DepthToGW", "C_Mo", "Top5_Ca", "A_Tot_14A", "C_Amorph", "C_Analcime")
#define predictor and response variables in training set, As= 10 ug/L, keep variables defined above
train_x = data.matrix(train[, c(3, 5, 29, 25, 99, 2, 17, 71, 27, 108, 80, 11, 58, 65, 66)])
train_y = train[,161]
#define predictor and response variables in testing set
test_x = data.matrix(test[, c(3, 5, 29, 25, 99, 2, 17, 71, 27, 108, 80, 11, 58, 65, 66)])
test_y = test[,161]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Set parameters from all the tuning, steps 2 and 3
params = list(alpha = 2,
lambda = 5,
gamma = 1,
max_delta_step = 1,
eta = 0.005,
max_depth = 6,
subsample = 0.50,
colsample_bytree = 0.75,
min_child_weight = 1,
booster = "gbtree")
#Fully tuned model
model = xgboost(data = xgb_train, params = params,
nrounds = 750, objective = "binary:logistic",
eval_metric = "error", verbose = 1,
print_every_n = 100)
#write.csv(dfAc, file="20241223_modelTuning_primaryHyperparameters_alpha2Lambda5.csv")
#Testing Data
xgbpred <- predict (model, xgb_test)
xgbpred2 <- ifelse (xgbpred > 0.5,1,0)
confusionMatrix (factor(xgbpred2), factor(test_y)) #keep this for reporting
#Adjust the "true" threshold using Youden value
#For a figure
y_predJoin<-data.frame(cbind(test_y, xgbpred))#change field to match outcome modeled, this applies to LT10
#rename fields for ease of use
colnames(y_predJoin)[1]<-"Obsclass"
colnames(y_predJoin)[2]<-"Predexceed"
#Use cutpoint to identify threshold for As 'detection' balancing sensitivity and specificity using Youden metric
cp <- cutpointr(y_predJoin, Predexceed, Obsclass,
method = maximize_metric, metric = youden, pot_class = 1)
summary(cp) #make note of the cutpoint value for comparision with lines 91-93 above
plot(cp)
#Extract ROC Curve data for plotting
a<-as.data.frame(cp$roc_curve)
a$sens<-a$tp/(a$tp+a$fn) #sensitivity
a$spec<-a$tn/(a$tn+a$fp) #specificity
a$j<-(a$tp/(a$tp+a$fn))+(a$tn/(a$tn+a$fp))-1 #j-index, also called Youden value
##Make a plot like USGS PFAS paper S8
df <- a %>%
select(x.sorted, j, sens, spec) %>%
gather(key = "variable", value = "value", -x.sorted)
##Make a plot like USGS PFAS paper S8
library(tidyverse)
df <- a %>%
select(x.sorted, j, sens, spec) %>%
gather(key = "variable", value = "value", -x.sorted)
ggplot(df, aes(x = x.sorted, y = value)) +
geom_line(aes(color = variable, linetype = variable)) +
scale_color_manual(values = c("black","darkred", "steelblue")) +
xlab("As Detection Threshold - value above this threshold is considered a detection") + ylab("Metric Estimate")
shap_long <- shap.prep(xgb_model = model, X_train = train_x)
# **SHAP summary plot**
shap.plot.summary(shap_long)
#Load raster files for prediction model
wd <- ("/Users/hoover/desktop/")
rasterlist2 <-  list.files(paste0(wd,"spatialPredFormattedTifs"), full.names=TRUE, pattern=".tif$")
rasterlist2
d<-"/Users/hoover/desktop/spatialPredFormattedTifs/"
#Load each raster to check extent and crop as needed
A_Aragon<-raster(paste(d, "A_Aragon.tif", sep=""))
A_Calcite<-raster(paste(d,"A_Calcite.tif", sep=""))
A_Cs<-raster(paste(d,"A_Cs.tif", sep=""))
A_Tot_14A<-raster(paste(d,"A_Tot_14A.tif", sep=""))
C_Amorph<-raster(paste(d,"C_Amorph.tif", sep=""))
C_Analcime<-raster(paste(d,"C_Analcime.tif", sep=""))
C_Cr<-raster(paste(d,"C_Cr.tif", sep=""))
C_Hematite<-raster(paste(d,"C_Hematite.tif", sep=""))
C_Mo<-raster(paste(d,"C_Mo.tif", sep=""))
Fe<-raster(paste(d,"Fe.tif", sep=""))
pH<-raster(paste(d,"pH.tif", sep=""))
prism30yr<-raster(paste(d,"prism30yr.tif", sep=""))
Top5_Ca<-raster(paste(d,"Top5_Ca.tif", sep=""))
Top5_S<-raster(paste(d,"Top5_S.tif", sep=""))
DepthToGW<-raster(paste(d,"DepthToGW.tif", sep=""))
#Change names so they match the XGB model
A_Aragon@data@names<-"A_Aragon"
A_Calcite@data@names<-"A_Calcite"
A_Cs@data@names<-"A_Cs"
A_Tot_14A@data@names<-"A_Tot_14A"
C_Amorph@data@names<-"C_Amorph"
C_Analcime@data@names<-"C_Analcime"
C_Cr@data@names<-"C_Cr"
C_Hematite@data@names<-"C_Hematite"
C_Mo@data@names<-"C_Mo"
Fe@data@names<-"Fe"
pH@data@names<-"pH"
prism30yr@data@names<-"prism30yr"
Top5_Ca@data@names<-"Top5_Ca"
Top5_S@data@names<-"Top5_S"
DepthToGW@data@names<-"DepthToGW"
# create raster stack and convert to a maxtrix so it works with predict function for XGB
rstack1 <- stack(pH, prism30yr, A_Cs, A_Aragon, C_Hematite, Fe, Top5_S,
C_Cr, A_Calcite, DepthToGW, C_Mo, Top5_Ca, A_Tot_14A,
C_Amorph,C_Analcime)
rstack2<-rasterToPoints(rstack1)
spatialPred <- as.data.frame(predict (model, rstack2[,-c(1,2)]))
colnames(spatialPred)[1]<-"AsPredict"
rstack3<-as.data.frame(rstack2)
rstack3$AsPred<-spatialPred$AsPredict
#Convert to raster
#crs<-paste(Fe@srs)
r<-rasterFromXYZ(rstack3[,c(1,2,18)], res=c(500,500))
#Make a plot and write to file
plot(r)
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
library(cutpointr)
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
#setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
# load libraries
library(caTools)
library(caret)
library(gbm)
library(xgboost) # for xgboost
library("SHAPforxgboost")
library(data.table)
library(cutpointr)
rm(list=ls())
# set data and seed values
date<-Sys.Date()
set.seed(1234)  # Setting seed
setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
#setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data/")
#Load data
#Asdata = read.csv(in_path, na.strings = "NULL")
Asdata = read.csv("All_As_Data.csv", na.strings = "NULL")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE5_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Make a list of the fewest number of variables with the highest overall prediction accuracy
a<-list("pH", "Fe", "prism30yr", "A_Calcite", "DepthToGW", "A_Kaolinit", "C_Se", "C_Sb", "A_Quartz",
"Top5_Ca", "A_Tot_Flds", "C_Hematite", "C_Tot_14A", "A_Hg", "A_Tl", "A_C_Tot", "C_Cr", "C_Kaolinit",
"Top5_As", "Top5_Ba")
#define predictor and response variables in training set, As= 5 ug/L, keep variables defined above
train_x = data.matrix(train[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9)])
train_y = train[,160]
#define predictor and response variables in testing set
test_x = data.matrix(test[, c(3,2,5,27,108,38,88,87,47,11,60,99,106,36,56,26,71,102,8,9)])
test_y = test[,160]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
#Set parameters from all the tuning, steps 2 and 3
params = list(alpha = 0,
lambda = 1,
gamma = 2,
max_delta_step = 0,
eta = 0.001,
max_depth = 4,
subsample = 0.50,
colsample_bytree = 0.75,
min_child_weight = 1,
booster = "gbtree")
#Fully tuned model
model = xgboost(data = xgb_train, params = params,
nrounds = 1000, objective = "binary:logistic",
eval_metric = "error", verbose = 1,
print_every_n = 100)
#write.csv(dfAc, file="20241223_modelTuning_primaryHyperparameters_alpha2Lambda5.csv")
#Testing Data
xgbpred <- predict (model, xgb_test)
xgbpred2 <- ifelse (xgbpred > 0.5,1,0)
confusionMatrix (factor(xgbpred2), factor(test_y)) #keep this for reporting
#Adjust the "true" threshold using Youden value
#For a figure
y_predJoin<-data.frame(cbind(test_y, xgbpred))#change field to match outcome modeled, this applies to LT10
#rename fields for ease of use
colnames(y_predJoin)[1]<-"Obsclass"
colnames(y_predJoin)[2]<-"Predexceed"
#Use cutpoint to identify threshold for As 'detection' balancing sensitivity and specificity using Youden metric
cp <- cutpointr(y_predJoin, Predexceed, Obsclass,
method = maximize_metric, metric = youden, pot_class = 1)
summary(cp) #make note of the cutpoint value for comparision with lines 91-93 above
plot(cp)
#Extract ROC Curve data for plotting
a<-as.data.frame(cp$roc_curve)
a$sens<-a$tp/(a$tp+a$fn) #sensitivity
a$spec<-a$tn/(a$tn+a$fp) #specificity
a$j<-(a$tp/(a$tp+a$fn))+(a$tn/(a$tn+a$fp))-1 #j-index, also called Youden value
##Make a plot like USGS PFAS paper S8
df <- a %>%
select(x.sorted, j, sens, spec) %>%
gather(key = "variable", value = "value", -x.sorted)
ggplot(df, aes(x = x.sorted, y = value)) +
geom_line(aes(color = variable, linetype = variable)) +
scale_color_manual(values = c("black","darkred", "steelblue")) +
xlab("As Detection Threshold - value above this threshold is considered a detection") + ylab("Metric Estimate")
# To prepare the long-format data:
shap_long <- shap.prep(xgb_model = model, X_train = train_x)
# **SHAP summary plot**
shap.plot.summary(shap_long)
#Load raster files for prediction model
#for spatial data
library(raster)
library(sp)
library(terra)
wd <- ("/Users/hoover/desktop/")
rasterlist2 <-  list.files(paste0(wd,"spatialPredFormattedTifs"), full.names=TRUE, pattern=".tif$")
rasterlist2
d<-"/Users/hoover/desktop/spatialPredFormattedTifs/"
#Load each raster to check extent and crop as needed
A_C_Tot<-raster(paste(d,"A_C_Tot.tif", sep=""))
A_Calcite<-raster(paste(d,"A_Calcite.tif", sep=""))
A_Hg<-raster(paste(d,"A_Hg.tif", sep=""))
A_Kaolinit<-raster(paste(d,"A_Kaolinit.tif", sep=""))
A_Quartz<-raster(paste(d,"A_Quartz.tif", sep=""))
A_Tl<-raster(paste(d,"A_Tl.tif", sep=""))
A_Tot_Flds<-raster(paste(d,"A_Tot_Flds.tif", sep=""))
C_Cr<-raster(paste(d,"C_Cr.tif", sep=""))
C_Hematite<-raster(paste(d,"C_Hematite.tif", sep=""))
C_Kaolinit<-raster(paste(d,"C_Kaolinit.tif", sep=""))
C_Sb<-raster(paste(d,"C_Sb.tif", sep=""))
C_Se<-raster(paste(d,"C_Se.tif", sep=""))
C_Cr<-raster(paste(d,"C_Cr.tif", sep=""))
C_Hematite<-raster(paste(d,"C_Hematite.tif", sep=""))
#C_Kaolinit<-raster(paste(d,"C_Kaolinit.tif", sep=""))
C_Sb<-raster(paste(d,"C_Sb.tif", sep=""))
C_Se<-raster(paste(d,"C_Se.tif", sep=""))
#C_Tot_14A<-raster(paste(d,"C_Tot_14A.tif", sep=""))
DepthToGW<-raster(paste(d,"DepthToGW.tif", sep=""))
Fe<-raster(paste(d,"Fe.tif", sep=""))
pH<-raster(paste(d,"pH.tif", sep=""))
prism30yr<-raster(paste(d,"prism30yr.tif", sep=""))
Top5_As<-raster(paste(d,"Top5_As.tif", sep=""))
Top5_Ba<-raster(paste(d,"Top5_Ba.tif", sep=""))
Top5_Ca<-raster(paste(d,"Top5_Ca.tif", sep=""))
#Change names so they match the XGB model
A_C_Tot@data@names<-"A_C_Tot"
A_Calcite@data@names<-"A_Calcite"
A_Hg@data@names<-"A_Hg"
A_Kaolinit@data@names<-"A_Kaolinit"
A_Quartz@data@names<-"A_Quartz"
A_Tl@data@names<-"A_Tl"
A_Tot_Flds@data@names<-"A_Tot_Flds"
C_Cr@data@names<-"C_Cr"
C_Hematite@data@names<-"C_Hematite"
C_Kaolinit@data@names<-"C_Kaolinit"
#Change names so they match the XGB model
A_C_Tot@data@names<-"A_C_Tot"
A_Calcite@data@names<-"A_Calcite"
A_Hg@data@names<-"A_Hg"
A_Kaolinit@data@names<-"A_Kaolinit"
A_Quartz@data@names<-"A_Quartz"
A_Tl@data@names<-"A_Tl"
A_Tot_Flds@data@names<-"A_Tot_Flds"
C_Cr@data@names<-"C_Cr"
C_Hematite@data@names<-"C_Hematite"
#C_Kaolinit@data@names<-"C_Kaolinit"
C_Sb@data@names<-"C_Sb"
C_Se@data@names<-"C_Se"
#C_Tot_14A@data@names<-"C_Tot_14A"
DepthToGW@data@names<-"DepthToGW"
Fe@data@names<-"Fe"
pH@data@names<-"pH"
prism30yr@data@names<-"prism30yr"
Top5_As@data@names<-"Top5_As"
Top5_Ba@data@names<-"Top5_Ba"
Top5_Ca@data@names<-"Top5_Ca"
model$feature_names
