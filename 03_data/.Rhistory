AsTrain<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
AsTrainM<-as.matrix.data.frame(AsTrain)
AsTrainM<-as.matrix(AsTrain)
AsTrainM<-data.matrix(AsTrain)
str(AsTrainM)
str(AsTest)
str(AsTrain)
str(AsTrain[,50:100])
str(AsTrain[,100:150])
str(AsTrain[,100:68])
str(AsTrain[,1:168])
str(AsTrain[,2:168])
str(AsTrain[,1:167])
str(AsTrain[])
str(AsTrain[,c(1:75)])
AsTrainM<-as.numeric(AsTrain)
AsTrain<-unlist(train[,-c(1, 4, 109:112, 157:160, 162:168)]) #Drop the As concentration, and the categorical variables we already transformed
AsTrainM<-as.matrix(AsTrain)
View(AsTrainM)
AsTrain<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
AsTrainM<-unlist(AsTrain)
matrix<-data.matrix(AsTrain)
View(matrix)
matrix<-data.matrix(AsTrain, rownames.force = NA)
View(matrix)
matrix2<-as.matrix(sapply(AsTrain, as.numeric))
View(matrix2)
str(matrix2)
matrix<-data.matrix(AsTrain, stringsAsFactors = FALSE)
matrix<-data.matrix(AsTrain)
head(AsTrainM)
matrix<-data.matrix(AsTrain[,c(1:50)])
View(matrix)
matrix<-data.matrix(AsTrain[,c(1:10)])
matrix<-data.matrix(AsTrain[,c(1:20)])
matrix<-data.matrix(AsTrain[,c(1:30)])
View(matrix)
matrix<-data.matrix(AsTrain[,c(1:40)])
matrix<-data.matrix(AsTrain)
shap_values <- shap.values(xgb_model = Arsenic_xgb, X_train = matrix)
library(modeldata)
install.packages("modeldata")
install.packages("tidymodels")
install.packages("gt")
install.packages("vip")
# load packages
library(modeldata)
library(tidymodels)
library(tidyverse)
library(gt)
data("credit_data")
credit_data <- credit_data %>%
drop_na()
set.seed(12)
# initial split
split <- initial_split(credit_data, prop = 0.75, strata = "Status")
# train/test sets
train <- training(split)
test <- testing(split)
rec <- recipe(Status ~ ., data = train) %>%
step_bagimpute(Home, Marital, Job, Income, Assets, Debt) %>%
step_dummy(Home, Marital, Records, Job, one_hot = T)
# Just some sensible values, not optimised by any means!
mod <- boost_tree(trees = 500,
mtry = 6,
min_n = 10,
tree_depth = 5) %>%
set_engine("xgboost", eval_metric = 'error') %>%
set_mode("classification")
xgboost_wflow <- workflow() %>%
add_recipe(rec) %>%
add_model(mod) %>%
fit(train)
xg_res <- last_fit(xgboost_wflow,
split,
metrics = metric_set(roc_auc, pr_auc, accuracy))
rec <- recipe(Status ~ ., data = train) %>%
step_bagimpute(Home, Marital, Job, Income, Assets, Debt) %>%
step_dummy(Home, Marital, Records, Job, one_hot = T)
rec <- recipe(Status ~ ., data = train) %>%
step_impute_bag(Home, Marital, Job, Income, Assets, Debt) %>%
step_dummy(Home, Marital, Records, Job, one_hot = T)
# Just some sensible values, not optimised by any means!
mod <- boost_tree(trees = 500,
mtry = 6,
min_n = 10,
tree_depth = 5) %>%
set_engine("xgboost", eval_metric = 'error') %>%
set_mode("classification")
xgboost_wflow <- workflow() %>%
add_recipe(rec) %>%
add_model(mod) %>%
fit(train)
xg_res <- last_fit(xgboost_wflow,
split,
metrics = metric_set(roc_auc, pr_auc, accuracy))
xg_res <- last_fit(xgboost_wflow,
split,
metrics = metric_set(roc_auc, pr_auc, accuracy))
preds <- xg_res %>%
collect_predictions()
install.packages("fastshap")
library(fastshap)
?explain
# Compute shapley values
shap <- explain(Arsenic_xgb, X = matrix2, exact = TRUE)
library(shapviz)
library(ggplot2)
library(xgboost)
set.seed(3653)
X <- diamonds[c("carat", "cut", "color", "clarity")]
rm(list=ls())
X <- diamonds[c("carat", "cut", "color", "clarity")]
dtrain <- xgb.DMatrix(data.matrix(X), label = diamonds$price)
fit <- xgb.train(
params = list(learning_rate = 0.1, objective = "reg:squarederror"),
data = dtrain,
nrounds = 65L
)
# Explanation dataset
X_small <- X[sample(nrow(X), 2000L), ]
shp <- shapviz(fit, X_pred = data.matrix(X_small), X = X_small)
library(shapviz)
install.packages("shapviz")
shp <- shapviz(fit, X_pred = data.matrix(X_small), X = X_small)
library(shapviz)
shp <- shapviz(fit, X_pred = data.matrix(X_small), X = X_small)
sv_importance(shp)
sv_importance(shp, kind = "bar")
sv_importance(shp, kind = "both", alpha = 0.2, width = 0.2)
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
#Load XGB model
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE1_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-04_ClassLTE2_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE3_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE5_cv10_xgb.rds")
Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE10_cv10_xgb.rds")
Arsenic_xgb
# Filter data into train and test sets based on logical variable 'trainCat2'
#train <- Asdata[Asdata$trainClassLTE2_splt == TRUE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE1_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE2_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE3_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#Make SiteID the row name so we can drop that field
#rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Drop unused fields
#AsTrain<-train[,-c(1, 4, 109:112, 157, 159:168)] #Drop the As concentration, and the categorical variables we already transformed
#AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE1
#AsTest<-test[,-c(1, 4, 109:112, 157,159:168)] #for use with LTE2
#AsTest<-test[,-c(1, 4, 109:112, 157:158, 160:168)] #for use with LTE3
#AsTest<-test[,-c(1, 4, 109:112, 157:159, 161:168)] #for use with LTE5
AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE10
#Ensure ClassLTE2 is a Factor (Categorical Variable)
#AsTest$ClassLTE2 <- as.factor(AsTest$ClassLTE2)
#AsTest$ClassLTE3 <- as.factor(AsTest$ClassLTE3)
#AsTest$ClassLTE5 <- as.factor(AsTest$ClassLTE5)
AsTest$ClassLTE10  <- as.factor(AsTest$ClassLTE10)
#Load data
Asdata <- read.csv("All_As_Data.csv")
#Load XGB model
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE1_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-04_ClassLTE2_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE3_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE5_cv10_xgb.rds")
Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE10_cv10_xgb.rds")
Arsenic_xgb
# Filter data into train and test sets based on logical variable 'trainCat2'
#train <- Asdata[Asdata$trainClassLTE2_splt == TRUE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE1_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE2_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE3_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#Make SiteID the row name so we can drop that field
#rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Drop unused fields
#AsTrain<-train[,-c(1, 4, 109:112, 157, 159:168)] #Drop the As concentration, and the categorical variables we already transformed
#AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE1
#AsTest<-test[,-c(1, 4, 109:112, 157,159:168)] #for use with LTE2
#AsTest<-test[,-c(1, 4, 109:112, 157:158, 160:168)] #for use with LTE3
#AsTest<-test[,-c(1, 4, 109:112, 157:159, 161:168)] #for use with LTE5
AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE10
#Ensure ClassLTE2 is a Factor (Categorical Variable)
#AsTest$ClassLTE2 <- as.factor(AsTest$ClassLTE2)
#AsTest$ClassLTE3 <- as.factor(AsTest$ClassLTE3)
#AsTest$ClassLTE5 <- as.factor(AsTest$ClassLTE5)
AsTest$ClassLTE10  <- as.factor(AsTest$ClassLTE10)
# Predicting the Test set results
#Predictions
y_pred = predict(Arsenic_xgb, newdata = test[,-1], type="prob")
#y_pred = predict(Arsenic_xgb, newdata = test[,-1], missing ="NULL"); length(y_pred)
#Load data
#classifier_RF <- readRDS("2024-11-25_classLTE3_cv10_rf.rds")
#classifier_RF
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
AsTrain<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
shp <- shapviz(Arsenic_xgb, X_pred = data.matrix(AsTrain), X = AsTrain)
require(xgboost)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
rm(list=ls())
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
test <- agaricus.test
str(train)
dim(train$data)
dim(test$data)
class(train$data)[1]
class(train$label)
bstSparse <- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
bstDense <- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
dtrain <- xgb.DMatrix(data = train$data, label = train$label)
bstDMatrix <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
# verbose = 0, no message
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = 0)
# verbose = 1, print evaluation metric
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = 1)
# verbose = 2, also print information about tree
bst <- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic", verbose = 2)
pred <- predict(bst, test$data)
# size of the prediction vector
print(length(pred))
prediction <- as.numeric(pred > 0.5)
print(head(prediction))
err <- mean(as.numeric(pred > 0.5) != test$label)
print(paste("test-error=", err))
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
#Load XGB model
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE1_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-04_ClassLTE2_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE3_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE5_cv10_xgb.rds")
Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE10_cv10_xgb.rds")
Arsenic_xgb
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
#Drop unused fields
AsTrain<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
#Ensure ClassLTE2 is a Factor (Categorical Variable)
AsTrain$ClassLTE10 <- as.factor(AsTrain$ClassLTE10)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
trainM<-data.matrix(AsTrain)
dim(train$data)
dim(trainM)
class(train$data)
class(train$data)[1]
class(trainM$data)[1]
class(trainM$data)
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
a<-list(AsData)
a<-list(Asdata)
str(a)
a$data
Create example data frame
data <- data.frame(x1 = c(10,20,50,10,90,30,40),
x2 = c(50,20,10,80,90,100,60),
x3 = c(0,10,30,50,70,40,80))
#  print the dataframe
data
list <- split(data,seq(nrow(data)))
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')
train <- agaricus.train
train
head(agaricus.test)
#load the data
data = MASS::Boston
#view the structure of the data
str(data)
#split into training (80%) and testing set (20%)
parts = createDataPartition(data$medv, p = .8, list = F)
rm(list=ls())
data = MASS::Boston
#view the structure of the data
str(data)
#make this example reproducible
set.seed(0)
#split into training (80%) and testing set (20%)
parts = createDataPartition(data$medv, p = .8, list = F)
train = data[parts, ]
test = data[-parts, ]
#define predictor and response variables in training set
train_x = data.matrix(train[, -13])
train_y = train[,13]
#define predictor and response variables in testing set
test_x = data.matrix(test[, -13])
test_y = test[, 13]
#Load data
Asdata <- read.csv("All_As_Data.csv")
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Drop unused fields
Asdata2<-Asdata[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
View(Asdata2)
View(Asdata)
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Drop unused fields
trainAs<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
View(trainAs)
#define predictor and response variables in training set
train_x = data.matrix(trainAs[, -151])
train_y = trainAs[,151]
testAs<-test[,c(1, 4, 109:112, 157:160, 162:168)]
#define predictor and response variables in testing set
test_x = data.matrix(testAs[, -151])
test_y = testAs[, 151]
?data.matrix
summary(train_x)
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
test_y = testAs[, 151]
testAs<-test[,c(1, 4, 109:112, 157:160, 162:168)]
View(testAs)
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Drop unused fields
trainAs<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
testAs<-test[,-c(1, 4, 109:112, 157:160, 162:168)]
#define predictor and response variables in training set
train_x = data.matrix(trainAs[, -151])
train_y = trainAs[,151]
#define predictor and response variables in testing set
test_x = data.matrix(testAs[, -151])
test_y = testAs[, 151]
#define final training and testing sets
xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)
#define watchlist
watchlist = list(train=xgb_train, test=xgb_test)
Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE10_cv10_xgb.rds")
Arsenic_xgb
?xgb.train
param <- list(max_depth = 6, eta = 0.01, verbose = 1, colsample_bytree=0.25, nthread = 1,
min_child_weight = 1, subsample = 0.25, objective = "binary:logistic",
eval_metric = "accuracy")
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, params = param, watchlist=watchlist, nrounds = 1000)
param <- list(max_depth = 6, eta = 0.01, verbose = 1, colsample_bytree=0.25, nthread = 1,
min_child_weight = 1, subsample = 0.25, objective = "binary:logistic")
#fit XGBoost model and display training and testing data at each round
model = xgb.train(data = xgb_train, params = param, watchlist=watchlist, nrounds = 1000)
#fit XGBoost model and display training and testing data at each round
model = xgboost(data = xgb_train, params = param, watchlist=watchlist, nrounds = 1000)
#fit XGBoost model and display training and testing data at each round
model = xgboost(data = xgb_train, nrounds = 1000)
?xgboost
#fit XGBoost model and display training and testing data at each round
model = xgboost(data = xgb_train, params = param, nrounds = 1000)
#fit XGBoost model and display training and testing data at each round
model = xgboost(data = xgb_train, params = param, watchlist=watchlist, nrounds = 1000)
library(tidyverse)
library(here)
install.packages("here")
library(here)
library(xgboost)
wine <- read_csv(here("data", "wine.csv"))
rm(list=ls())
wine <- read_csv(here("data", "wine.csv"))
wine <- read_csv(here("data", "wine.csv"))
redwine <- wine %>% dplyr::slice(1:1599)
library(tidyverse)
library(here)
library(xgboost)
wine <- read_csv(here("data", "wine.csv"))
redwine <- wine %>% dplyr::slice(1:1599)
rm(list=ls())
setwd("/Users/hoover/Documents/GitHub/coPlateauWaterQuality/03_data/")
#setwd("/Users/aaronnuanez/Documents/GitHub/coPlateauWaterQuality/03_data")
rm(list=ls())
#Load data
Asdata <- read.csv("All_As_Data.csv")
# Filter data into train and test sets based on logical variable
train <- Asdata[Asdata$trainClassLTE10_splt == TRUE, ]
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ]
#Drop unused fields
trainAs<-train[,-c(1, 4, 109:112, 157:160, 162:168)] #Drop the As concentration, and the categorical variables we already transformed
testAs<-test[,-c(1, 4, 109:112, 157:160, 162:168)]
#Load XGB model
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE1_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-04_ClassLTE2_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE3_cv10_xgb.rds")
#Arsenic_xgb<-readRDS("./XGB_rds/2024-12-05_ClassLTE5_cv10_xgb.rds")
Arsenic_xgb<-readRDS("./XGB_rds/2024-12-08_ClassLTE10_cv10_xgb.rds")
Arsenic_xgb
#Make SiteID the row name so we can drop that field
rownames(train)<-train$SiteID
# Filter data into train and test sets based on logical variable 'trainCat2'
#train <- Asdata[Asdata$trainClassLTE2_splt == TRUE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE1_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE2_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE3_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#test <- Asdata[Asdata$trainClassLTE5_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
test <- Asdata[Asdata$trainClassLTE10_splt == FALSE, ] #Need up update this field and dataframe to match what is produce in lines 21-24
#Make SiteID the row name so we can drop that field
#rownames(train)<-train$SiteID
rownames(test)<-test$SiteID
#Drop unused fields
#AsTrain<-train[,-c(1, 4, 109:112, 157, 159:168)] #Drop the As concentration, and the categorical variables we already transformed
#AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE1
#AsTest<-test[,-c(1, 4, 109:112, 157,159:168)] #for use with LTE2
#AsTest<-test[,-c(1, 4, 109:112, 157:158, 160:168)] #for use with LTE3
#AsTest<-test[,-c(1, 4, 109:112, 157:159, 161:168)] #for use with LTE5
AsTest<-test[,-c(1, 4, 109:112, 157:160, 162:168)] #for use with LTE10
#Ensure ClassLTE2 is a Factor (Categorical Variable)
#AsTest$ClassLTE2 <- as.factor(AsTest$ClassLTE2)
#AsTest$ClassLTE3 <- as.factor(AsTest$ClassLTE3)
#AsTest$ClassLTE5 <- as.factor(AsTest$ClassLTE5)
AsTest$ClassLTE10  <- as.factor(AsTest$ClassLTE10)
# Predicting the Test set results
#Predictions
y_pred = predict(Arsenic_xgb, newdata = test[,-1], type="prob")
#Join probability with outcome from Test set
#y_predJoin<-cbind(test[,157], y_pred) #change field to match outcome modeled, this applies to LT1
#y_predJoin<-cbind(test[,158], y_pred) #change field to match outcome modeled, this applies to LT2
#y_predJoin<-cbind(test[,159], y_pred) #change field to match outcome modeled, this applies to LT3
#y_predJoin<-cbind(test[,160], y_pred) #change field to match outcome modeled, this applies to LT5
y_predJoin<-cbind(test[,161], y_pred) #change field to match outcome modeled, this applies to LT10
#rename fields for ease of use
colnames(y_predJoin)[1]<-"Obsclass"
colnames(y_predJoin)[2]<-"PrednoExceed"
colnames(y_predJoin)[3]<-"Predexceed"
#Use cutpoint to identify threshold for As 'detection' balancing sensitivity and specificity using Youden metric
cp <- cutpointr(y_predJoin, Predexceed, Obsclass,
method = maximize_metric, metric = youden)
summary(cp)
plot(cp)
#Extract ROC Curve data for plotting
a<-as.data.frame(cp$roc_curve)
a$sens<-a$tp/(a$tp+a$fn) #sensitivity
a$spec<-a$tn/(a$tn+a$fp) #specificity
a$j<-(a$tp/(a$tp+a$fn))+(a$tn/(a$tn+a$fp))-1 #j-index, also called Youden value
##Make a plot like USGS PFAS paper S8
df <- a %>%
select(x.sorted, j, sens, spec) %>%
gather(key = "variable", value = "value", -x.sorted)
ggplot(df, aes(x = x.sorted, y = value)) +
geom_line(aes(color = variable, linetype = variable)) +
scale_color_manual(values = c("black","darkred", "steelblue")) +
xlab("As Detection Threshold - value above this threshold is considered a detection") + ylab("Metric Estimate")
#Assign predicted class using ROC threshold from previous steps
y_pred$class<-0
#y_pred$class[y_pred$'1' > 0.1258]<-1 #for LTE1
#y_pred$class[y_pred$'1' > 0.6034]<-1 #for LTE2
#y_pred$class[y_pred$'1' > 0.6023]<-1 #for LTE3
#y_pred$class[y_pred$'1' > 0.1405]<-1 #for LTE5
y_pred$class[y_pred$'1' > 0.1259]<-1 #for LTE10
#
# Confusion Matrix
#confusion_mtx <- confusionMatrix(factor(y_pred$class), AsTest$ClassLTE1, positive = '1')
#confusion_mtx <- confusionMatrix(factor(y_pred$class), AsTest$ClassLTE2, positive = '1')
#confusion_mtx <- confusionMatrix(factor(y_pred$class), AsTest$ClassLTE3, positive = '1')
#confusion_mtx <- confusionMatrix(factor(y_pred$class), AsTest$ClassLTE5, positive = '1')
confusion_mtx <- confusionMatrix(factor(y_pred$class), AsTest$ClassLTE10, positive = '1')
confusion_mtx
# Plotting model
plot(Arsenic_xgb)
# Calculate Accuracy
accuracy <- confusion_mtx$overall['Accuracy']
accuracy
# Calculate kappa value
kappa_value <- confusion_mtx$overall['Kappa']
kappa_value
# Extract Sensitivity and Specificity for each class
sensitivity <- confusion_mtx$byClass[[1]]
sensitivity
specificity <- confusion_mtx$byClass[[2]]
specificity
# training data accuracy and kappa
# AvgAcc = accuracy  Avgkap = kappa
Arsenic_xgb$resample %>%
arrange(Resample) %>%
mutate(AvgAcc = mean(Accuracy)) %>%
mutate(Avgkap = mean(Kappa))
# Test data values
accuracy
kappa_value
sensitivity
specificity
importance <- varImp(Arsenic_xgb, scale = TRUE)
# Plot variable importance
plot(importance, top = 10, col = "blue",  main = "Variable Importance, XGB, As > 3ug/L")
# Install necessary packages
install.packages(c("xgboost", "iml", "ggplot2"))
